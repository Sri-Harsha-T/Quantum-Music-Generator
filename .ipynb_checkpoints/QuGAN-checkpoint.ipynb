{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/PennyLaneAI/pennylane\n",
      "  Cloning https://github.com/PennyLaneAI/pennylane to c:\\users\\sriha\\appdata\\local\\temp\\pip-req-build-b9lxnzy1\n",
      "  Resolved https://github.com/PennyLaneAI/pennylane to commit 595ce24c9cf04c1b7f3280ca14336f5b21c47ecf\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: toml in c:\\users\\sriha\\anaconda3\\lib\\site-packages (from PennyLane==0.27.0.dev0) (0.10.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sriha\\anaconda3\\lib\\site-packages (from PennyLane==0.27.0.dev0) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\sriha\\anaconda3\\lib\\site-packages (from PennyLane==0.27.0.dev0) (1.7.3)\n",
      "Collecting retworkx\n",
      "  Downloading retworkx-0.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting semantic-version>=2.7\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting autograd\n",
      "  Using cached autograd-1.5-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: cachetools in c:\\users\\sriha\\anaconda3\\lib\\site-packages (from PennyLane==0.27.0.dev0) (4.2.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sriha\\anaconda3\\lib\\site-packages (from PennyLane==0.27.0.dev0) (2.7.1)\n",
      "Collecting pennylane-lightning>=0.26\n",
      "  Downloading PennyLane_Lightning-0.26.1-cp39-cp39-win_amd64.whl (4.4 MB)\n",
      "Requirement already satisfied: appdirs in c:\\users\\sriha\\anaconda3\\lib\\site-packages (from PennyLane==0.27.0.dev0) (1.4.4)\n",
      "Collecting autoray>=0.3.1\n",
      "  Using cached autoray-0.5.1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\sriha\\anaconda3\\lib\\site-packages (from PennyLane==0.27.0.dev0) (2.27.1)\n",
      "Collecting zstd\n",
      "  Downloading zstd-1.5.2.6-cp39-cp39-win_amd64.whl (148 kB)\n",
      "Collecting ninja\n",
      "  Using cached ninja-1.10.2.4-py2.py3-none-win_amd64.whl (293 kB)\n",
      "Requirement already satisfied: future>=0.15.2 in c:\\users\\sriha\\anaconda3\\lib\\site-packages (from autograd->PennyLane==0.27.0.dev0) (0.18.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sriha\\anaconda3\\lib\\site-packages (from requests->PennyLane==0.27.0.dev0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sriha\\anaconda3\\lib\\site-packages (from requests->PennyLane==0.27.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sriha\\anaconda3\\lib\\site-packages (from requests->PennyLane==0.27.0.dev0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sriha\\anaconda3\\lib\\site-packages (from requests->PennyLane==0.27.0.dev0) (3.3)\n",
      "Collecting rustworkx==0.12.0\n",
      "  Downloading rustworkx-0.12.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Building wheels for collected packages: PennyLane\n",
      "  Building wheel for PennyLane (PEP 517): started\n",
      "  Building wheel for PennyLane (PEP 517): finished with status 'done'\n",
      "  Created wheel for PennyLane: filename=PennyLane-0.27.0.dev0-py3-none-any.whl size=1130081 sha256=abaa3505d2f0b5486c455abe3157e060871bb5edee25f4ba8b88562b7e8963f2\n",
      "  Stored in directory: C:\\Users\\sriha\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-d4r7lsex\\wheels\\ff\\98\\7f\\b2d5531cd099e9cf4a7f4098f7fe1c7eca2f285847a57dbdb4\n",
      "Successfully built PennyLane\n",
      "Installing collected packages: rustworkx, ninja, zstd, semantic-version, retworkx, pennylane-lightning, dill, autoray, autograd, PennyLane\n",
      "Successfully installed PennyLane-0.27.0.dev0 autograd-1.5 autoray-0.5.1 dill-0.3.6 ninja-1.10.2.4 pennylane-lightning-0.26.1 retworkx-0.12.0 rustworkx-0.12.0 semantic-version-2.10.0 zstd-1.5.2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/PennyLaneAI/pennylane 'C:\\Users\\sriha\\AppData\\Local\\Temp\\pip-req-build-b9lxnzy1'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/PennyLaneAI/pennylane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpennylane\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtemplates\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AmplitudeEmbedding\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpennylane\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmusic21\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m converter, instrument,  note, chord, stream\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "\n",
    "from pennylane.templates.layers import BasicEntanglerLayers, StronglyEntanglingLayers, RandomLayers\n",
    "from pennylane.templates.embeddings import AmplitudeEmbedding\n",
    "from pennylane import numpy as np\n",
    "import torch\n",
    "from music21 import converter, instrument,  note, chord, stream\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle, glob\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriha\\AppData\\Local\\Temp\\ipykernel_19784\\3678011438.py:9: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  running_dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_wires = 12\n",
    "wires_range = range(n_wires)\n",
    "\n",
    "n_note_encoding = 7\n",
    "encoding_range = range(n_note_encoding)\n",
    "\n",
    "dev = qml.device('default.qubit', wires=n_wires)\n",
    "\n",
    "running_dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "running_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msilib import sequence\n",
    "\n",
    "\n",
    "notes_dir = 'notes.pk'\n",
    "frequencies_dir = 'frequencies.pk'\n",
    "\n",
    "\n",
    "class Midi:\n",
    "\n",
    "    def __init__(self, seq_length, device):\n",
    "        self.seq_length = seq_length\n",
    "        self.device = device\n",
    "\n",
    "        if Path(notes_dir).is_file():\n",
    "            self.notes = pickle.load(open(notes_dir, 'rb'))\n",
    "            self.frequencies = pickle.load(open(frequencies_dir, 'rb'))\n",
    "            # print(self.notes)\n",
    "            # print(self.frequencies)\n",
    "        else:\n",
    "            self.notes, self.frequencies = self.get_notes()\n",
    "            # print(self.notes)\n",
    "            # print(self.frequencies)\n",
    "            pickle.dump(self.notes, open(notes_dir, 'wb'))\n",
    "            pickle.dump(self.frequencies, open(frequencies_dir, 'wb'))\n",
    "\n",
    "        self.network_input, self.network_output = self.prepare_sequences(self.notes, self.frequencies)\n",
    "        print(f\"Input shape: {self.network_input.shape}\")\n",
    "        print(f\"Output shape: {self.network_output.shape}\")\n",
    "\n",
    "\n",
    "    def lazy_superimpose(self, input_chord):\n",
    "        frequencies = np.array([note.pitch.frequency for note in input_chord.notes])\n",
    "        return np.average(frequencies, axis=0)\n",
    "\n",
    "    def get_notes(self):\n",
    "        '''Get all the notes and chords from the midi files in the ./midi_song directory'''\n",
    "        # This is assuming that every interval between notes is the same (0.5)\n",
    "        notes = []\n",
    "        frequencies = []\n",
    "\n",
    "        for file in glob.glob(\"midi_songs/*.mid\"):\n",
    "            midi = converter.parse(file)\n",
    "\n",
    "            print(\"Parsing %s\" % file)\n",
    "\n",
    "            notes_to_parse = None\n",
    "\n",
    "            try:\n",
    "                s2 = instrument.partitionByInstrument(midi)\n",
    "                notes_to_parse = s2.parts[0].recurse()\n",
    "\n",
    "            except:\n",
    "                notes_to_parse = midi.flat.notes\n",
    "\n",
    "            \n",
    "            for element in notes_to_parse:\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                    frequencies.append(element.pitch.frequency)\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append(\".\".join(str(n) for n in element.normalOrder))\n",
    "                    frequencies.append(self.lazy_superimpose(element))\n",
    "                # print(element)\n",
    "        # print(\"Notes:\", notes)\n",
    "        # print(\"Freqs\", frequencies)\n",
    "        return notes, frequencies\n",
    "\n",
    "\n",
    "    def prepare_sequences(self, notes, frequencies):\n",
    "        '''Prepare the sequences used by the Neural Network'''\n",
    "\n",
    "        # Order pitchnames by their frequencties, so that the mse loss makes more sense\n",
    "        pitchnames = list(dict.fromkeys([x for _, x in sorted(zip(frequencies, notes))]))\n",
    "\n",
    "        # Dicttionary to map pitches to integers\n",
    "        self.note_to_int = {note : number for number, note in enumerate(pitchnames)}\n",
    "        self.int_to_note = {number: note for number, note in enumerate(pitchnames)}\n",
    "\n",
    "        network_input = []\n",
    "        network_output = []\n",
    "\n",
    "        # Create inoput sequences and the corresponding outpus \n",
    "        for i in range(len(self.notes) - self.seq_length):\n",
    "            sequence_in = self.notes[i : i + self.seq_length]\n",
    "            sequence_out = self.notes[ i + self.seq_length]\n",
    "            network_input.append([self.note_to_int[char] for char in sequence_in])\n",
    "            network_output.append(self.note_to_int[sequence_out])\n",
    "\n",
    "\n",
    "        n_patterns = len(network_input)\n",
    "        # print(n_patterns)\n",
    "\n",
    "        # reshape the input into a format compatible with LSTM Layers\n",
    "        # This is really (no. diff inputs, sequence length, no. of features)\n",
    "        network_input = np.reshape(network_input, (n_patterns, self.seq_length))\n",
    "        # print(network_input.shape)\n",
    "        network_input = network_input.tolist()\n",
    "\n",
    "        network_input = torch.tensor(network_input, device=self.device, dtype=torch.double)\n",
    "        network_output = torch.tensor(network_output, device=self.device)\n",
    "\n",
    "        self.input_norms = torch.tensor(torch.linalg.norm(network_input, axis=0))\n",
    "\n",
    "        return (\n",
    "            network_input, \n",
    "            network_output,\n",
    "        ) \n",
    "\n",
    "\n",
    "\n",
    "    def create_midi_from_model(self, prediction_output, filename):\n",
    "        '''convert the output from the prediction to notes and create a midi file from the notes'''\n",
    "        offset = 0\n",
    "        output_notes = []\n",
    "\n",
    "        # Create note and chord objects based on the values generated by the model\n",
    "        for pattern in prediction_output:\n",
    "            # pattern is a chord\n",
    "            if (\".\" in pattern) or pattern.isdigit():\n",
    "                notes_in_chord = pattern.split(\".\")\n",
    "                notes = []\n",
    "                for current_note in notes_in_chord:\n",
    "                    new_note=  note.Note(int(current_note))\n",
    "                    new_note.storedInstrument = instrument.Piano()\n",
    "                    notes.append(new_note)\n",
    "                new_chord = chord.Chord(notes)\n",
    "                new_chord.offset = offset\n",
    "                output_notes.append(new_chord)\n",
    "            # pattern is a note\n",
    "            else:\n",
    "                new_note = note.Note(pattern)\n",
    "                new_note.offset = offset\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                output_notes.append(new_note)\n",
    "            \n",
    "            # increase offset each iteration so that notes do not stack\n",
    "            offset += 0.5\n",
    "\n",
    "        midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "        midi_stream.write(\"midi\", fp=filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Midi\n",
      "Input shape: torch.Size([4877, 128])\n",
      "Output shape: torch.Size([4877])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriha\\AppData\\Local\\Temp\\ipykernel_19784\\542710876.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.input_norms = torch.tensor(torch.linalg.norm(network_input, axis=0))\n"
     ]
    }
   ],
   "source": [
    "seq_length = 2**n_note_encoding\n",
    "print(\"Initialized Midi\")\n",
    "midi = Midi(seq_length, running_dev)\n",
    "# notes = pickle.load(open(notes_dir, 'rb'))\n",
    "# print(notes)\n",
    "# freqs = pickle.load(open(frequencies_dir, 'rb'))\n",
    "# print(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_music(notes):\n",
    "    AmplitudeEmbedding(features=notes, wires=encoding_range, normalize=True)\n",
    "\n",
    "def music_generator(weights):\n",
    "    # StronglyEntanglingLayers(weights, wires=encoding_range)\n",
    "    # BasicEntanglerLayers(weights, wires=encoding_range)\n",
    "    RandomLayers(weights, wires=encoding_range)\n",
    "\n",
    "def discriminator(weights):\n",
    "    # BasicEntanglerLayers(weights, wires=wires_range)\n",
    "    StronglyEntanglingLayers(weights, wires=wires_range)\n",
    "\n",
    "def measurement(wire_count):\n",
    "    obs = qml.PauliZ(0)\n",
    "    for i in range(1, wire_count):\n",
    "        obs = obs @ qml.PauliZ(i)\n",
    "    return qml.expval(obs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface='torch')\n",
    "def real_music_dicriminator(inputs, weights):\n",
    "    encode_music(inputs)\n",
    "    discriminator(weights)\n",
    "    return measurement(n_note_encoding)\n",
    "\n",
    "def music_generator_circuit(inputs, note_weights):\n",
    "    encode_music(inputs)\n",
    "    music_generator(note_weights)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def generated_music_dicriminator(inputs, note_weights, weights):\n",
    "    music_generator_circuit(inputs, note_weights)\n",
    "    discriminator(weights)\n",
    "    return measurement(n_note_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_disc_layers = 12\n",
    "n_gen_layers = 20\n",
    "\n",
    "real_shapes = {'weights': (n_disc_layers, n_wires, 3)}\n",
    "real_layer  = qml.qnn.TorchLayer(real_music_dicriminator, real_shapes).to(running_dev)\n",
    "\n",
    "generated_shapes = {\n",
    "    'weights': (n_disc_layers, n_wires, 3), \n",
    "    'note_weights': (n_gen_layers, n_note_encoding),\n",
    "}\n",
    "\n",
    "generated_layer = qml.qnn.TorchLayer(generated_music_dicriminator, generated_shapes).to(running_dev)\n",
    "generated_layer.weights.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_weights(source_layer, target_layer):\n",
    "    '''Synchronize the weights of two layers'''\n",
    "    source_weights = source_layer.weights\n",
    "    target_weights = target_layer.weights\n",
    "    with torch.no_grad():\n",
    "        for source_weight, target_weight in zip(source_weights, target_weights):\n",
    "            target_weight.data = source_weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_fun_disc_true(layer):\n",
    "    def prob_true(inputs):\n",
    "        true_output = layer(inputs)\n",
    "        # Convert to probability\n",
    "        prob_true = (true_output + 1)/2\n",
    "        return prob_true\n",
    "        \n",
    "    return prob_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_real_true = prob_fun_disc_true(real_layer)\n",
    "prob_gen_true = prob_fun_disc_true(generated_layer)\n",
    "\n",
    "empty_input = torch.tensor(np.zeros((1, )).tolist()).to(running_dev)\n",
    "\n",
    "def disc_cost(inputs):\n",
    "    return prob_gen_true(inputs) - prob_real_true(inputs)\n",
    "\n",
    "def gen_cost(inputs):\n",
    "    return -prob_gen_true(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch_inputs(batch_size=1):\n",
    "    return midi.network_input[np.random.randint(0, len(midi.network_input), size=batch_size)\n",
    "    ]\n",
    "\n",
    "def shuffle_music(datapoint):\n",
    "    return datapoint[torch.randperm(datapoint.size()[0])].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_iteration(n_iterations, learning_rate):\n",
    "\n",
    "    opt = torch.optim.Adam(real_layer.parameters(), lr=learning_rate)\n",
    "    best_cost = disc_cost(midi.network_input[0])\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        opt.zero_grad()\n",
    "        # Sample a batch of data\n",
    "        batch_inputs = gen_batch_inputs()\n",
    "        batch_inputs = batch_inputs.detach()\n",
    "        # Compute the loss\n",
    "        loss = disc_cost(batch_inputs)\n",
    "        sync_weights(real_layer, generated_layer)\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        opt.step()\n",
    "        # Update the best cost\n",
    "        if loss < best_cost:\n",
    "            best_cost = loss\n",
    "        print('New Best Discriminator Cost:', best_cost)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_iteration(n_iterations, learning_rate):\n",
    "    opt = torch.optim.SGD(filter(lambda p: p.requires_grad, generated_layer.parameters()), lr=learning_rate)\n",
    "    best_cost = gen_cost(midi.network_input[0])\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        opt.zero_grad()\n",
    "        # Compute the loss\n",
    "\n",
    "        batch_inputs = gen_batch_inputs()\n",
    "        batch_inputs = shuffle_music(batch_inputs)\n",
    "\n",
    "        # print(generated_layer.note_weights)\n",
    "\n",
    "        loss = gen_cost(batch_inputs)\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        opt.step()\n",
    "        # Upadate the best cost\n",
    "        if loss < best_cost :\n",
    "            best_cost = loss\n",
    "\n",
    "    print('New Best Generator Cost:', best_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\math\\utils.py:61\u001b[0m, in \u001b[0;36mallclose\u001b[1;34m(a, b, rtol, atol, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# Some frameworks may provide their own allclose implementation.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# Try and use it if available.\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mallclose(a, b, rtol\u001b[39m=\u001b[39mrtol, atol\u001b[39m=\u001b[39matol, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m, \u001b[39mImportError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m     \u001b[39m# Otherwise, convert the input to NumPy arrays.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39m#    np.abs(a - b) <= atol + rtol * np.abs(b)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m#\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\autoray\\autoray.py:80\u001b[0m, in \u001b[0;36mdo\u001b[1;34m(fn, like, *args, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m backend \u001b[39m=\u001b[39m choose_backend(fn, \u001b[39m*\u001b[39margs, like\u001b[39m=\u001b[39mlike, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 80\u001b[0m \u001b[39mreturn\u001b[39;00m get_lib_fn(backend, fn)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: allclose(): argument 'other' (position 2) must be Tensor, not float",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sri Harsha\\IITB\\Autumn 2022\\Online\\Q-Magenta-Implementation\\QuGAN.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# print(\"Training model\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(steps):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         discriminator_iteration(n_iterations, learning_rate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39m# sync_weights(real_layer, generated_layer)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         generator_iteration(n_iterations, learning_rate)\n",
      "\u001b[1;32mc:\\Sri Harsha\\IITB\\Autumn 2022\\Online\\Q-Magenta-Implementation\\QuGAN.ipynb Cell 14\u001b[0m in \u001b[0;36mdiscriminator_iteration\u001b[1;34m(n_iterations, learning_rate)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiscriminator_iteration\u001b[39m(n_iterations, learning_rate):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     opt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(real_layer\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     best_cost \u001b[39m=\u001b[39m disc_cost(midi\u001b[39m.\u001b[39;49mnetwork_input[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_iterations):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         opt\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;32mc:\\Sri Harsha\\IITB\\Autumn 2022\\Online\\Q-Magenta-Implementation\\QuGAN.ipynb Cell 14\u001b[0m in \u001b[0;36mdisc_cost\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdisc_cost\u001b[39m(inputs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m prob_gen_true(inputs) \u001b[39m-\u001b[39m prob_real_true(inputs)\n",
      "\u001b[1;32mc:\\Sri Harsha\\IITB\\Autumn 2022\\Online\\Q-Magenta-Implementation\\QuGAN.ipynb Cell 14\u001b[0m in \u001b[0;36mprob_fun_disc_true.<locals>.prob_true\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprob_true\u001b[39m(inputs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     true_output \u001b[39m=\u001b[39m layer(inputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# Convert to probability\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     prob_true \u001b[39m=\u001b[39m (true_output \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\qnn\\torch.py:313\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(reconstructor)\n\u001b[0;32m    312\u001b[0m \u001b[39m# If the input is 1-dimensional, calculate the forward pass as usual\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate_qnode(inputs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\qnn\\torch.py:328\u001b[0m, in \u001b[0;36mTorchLayer._evaluate_qnode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m\"\"\"Evaluates the QNode for a single input datapoint.\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \n\u001b[0;32m    318\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39m    tensor: output datapoint\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    324\u001b[0m kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    325\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_arg: x},\n\u001b[0;32m    326\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{arg: weight\u001b[39m.\u001b[39mto(x) \u001b[39mfor\u001b[39;00m arg, weight \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqnode_weights\u001b[39m.\u001b[39mitems()},\n\u001b[0;32m    327\u001b[0m }\n\u001b[1;32m--> 328\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqnode(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mtype(x\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\qnode.py:611\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    608\u001b[0m         set_shots(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_device, override_shots)(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_gradient_fn)()\n\u001b[0;32m    610\u001b[0m \u001b[39m# construct the tape\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(args, kwargs)\n\u001b[0;32m    613\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecute_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    614\u001b[0m using_custom_cache \u001b[39m=\u001b[39m (\n\u001b[0;32m    615\u001b[0m     \u001b[39mhasattr\u001b[39m(cache, \u001b[39m\"\u001b[39m\u001b[39m__getitem__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    616\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(cache, \u001b[39m\"\u001b[39m\u001b[39m__setitem__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    617\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(cache, \u001b[39m\"\u001b[39m\u001b[39m__delitem__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    618\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\qnode.py:526\u001b[0m, in \u001b[0;36mQNode.construct\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39mtape\u001b[39m.\u001b[39mQuantumTape()\n\u001b[0;32m    525\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtape:\n\u001b[1;32m--> 526\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qfunc_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    527\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39m_qfunc_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qfunc_output\n\u001b[0;32m    529\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtape\u001b[39m.\u001b[39mget_parameters(trainable_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Sri Harsha\\IITB\\Autumn 2022\\Online\\Q-Magenta-Implementation\\QuGAN.ipynb Cell 14\u001b[0m in \u001b[0;36mgenerated_music_dicriminator\u001b[1;34m(inputs, note_weights, weights)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m@qml\u001b[39m\u001b[39m.\u001b[39mqnode(dev, interface\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerated_music_dicriminator\u001b[39m(inputs, note_weights, weights):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     music_generator_circuit(inputs, note_weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     discriminator(weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m measurement(n_note_encoding)\n",
      "\u001b[1;32mc:\\Sri Harsha\\IITB\\Autumn 2022\\Online\\Q-Magenta-Implementation\\QuGAN.ipynb Cell 14\u001b[0m in \u001b[0;36mmusic_generator_circuit\u001b[1;34m(inputs, note_weights)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmusic_generator_circuit\u001b[39m(inputs, note_weights):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     encode_music(inputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     music_generator(note_weights)\n",
      "\u001b[1;32mc:\\Sri Harsha\\IITB\\Autumn 2022\\Online\\Q-Magenta-Implementation\\QuGAN.ipynb Cell 14\u001b[0m in \u001b[0;36mencode_music\u001b[1;34m(notes)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_music\u001b[39m(notes):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     AmplitudeEmbedding(features\u001b[39m=\u001b[39;49mnotes, wires\u001b[39m=\u001b[39;49mencoding_range, normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\templates\\embeddings\\amplitude.py:129\u001b[0m, in \u001b[0;36mAmplitudeEmbedding.__init__\u001b[1;34m(self, features, wires, pad_with, normalize, do_queue, id)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_with \u001b[39m=\u001b[39m pad_with\n\u001b[0;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize \u001b[39m=\u001b[39m normalize\n\u001b[1;32m--> 129\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess(features, wires, pad_with, normalize)\n\u001b[0;32m    130\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(features, wires\u001b[39m=\u001b[39mwires, do_queue\u001b[39m=\u001b[39mdo_queue, \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39mid\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\templates\\embeddings\\amplitude.py:230\u001b[0m, in \u001b[0;36mAmplitudeEmbedding._preprocess\u001b[1;34m(features, wires, pad_with, normalize)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m normalize \u001b[39mor\u001b[39;00m pad_with:\n\u001b[0;32m    228\u001b[0m         feature_set \u001b[39m=\u001b[39m feature_set \u001b[39m/\u001b[39m qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39msqrt(norm)\n\u001b[1;32m--> 230\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m qml\u001b[39m.\u001b[39;49mmath\u001b[39m.\u001b[39;49mallclose(norm, \u001b[39m1.0\u001b[39;49m, atol\u001b[39m=\u001b[39;49mTOLERANCE):\n\u001b[0;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m normalize \u001b[39mor\u001b[39;00m pad_with:\n\u001b[0;32m    232\u001b[0m         feature_set \u001b[39m=\u001b[39m feature_set \u001b[39m/\u001b[39m qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39msqrt(norm)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\math\\utils.py:70\u001b[0m, in \u001b[0;36mallclose\u001b[1;34m(a, b, rtol, atol, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m     res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mallclose(a, b, rtol\u001b[39m=\u001b[39mrtol, atol\u001b[39m=\u001b[39matol, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m, \u001b[39mImportError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m     \u001b[39m# Otherwise, convert the input to NumPy arrays.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39m#    np.abs(a - b) <= atol + rtol * np.abs(b)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     t1 \u001b[39m=\u001b[39m ar\u001b[39m.\u001b[39;49mto_numpy(a)\n\u001b[0;32m     71\u001b[0m     t2 \u001b[39m=\u001b[39m ar\u001b[39m.\u001b[39mto_numpy(b)\n\u001b[0;32m     72\u001b[0m     res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mallclose(t1, t2, rtol\u001b[39m=\u001b[39mrtol, atol\u001b[39m=\u001b[39matol, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\autoray\\autoray.py:373\u001b[0m, in \u001b[0;36mto_numpy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_numpy\u001b[39m(x):\n\u001b[0;32m    372\u001b[0m     \u001b[39m\"\"\"Get a numpy version of array ``x``.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mreturn\u001b[39;00m do(\u001b[39m\"\u001b[39;49m\u001b[39mto_numpy\u001b[39;49m\u001b[39m\"\u001b[39;49m, x)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\autoray\\autoray.py:80\u001b[0m, in \u001b[0;36mdo\u001b[1;34m(fn, like, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m\"\"\"Do function named ``fn`` on ``(*args, **kwargs)``, peforming single\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mdispatch to retrieve ``fn`` based on whichever library defines the class of\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mthe ``args[0]``, or the ``like`` keyword argument if specified.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39m    <tf.Tensor: id=91, shape=(3, 3), dtype=float32>\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m backend \u001b[39m=\u001b[39m choose_backend(fn, \u001b[39m*\u001b[39margs, like\u001b[39m=\u001b[39mlike, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 80\u001b[0m \u001b[39mreturn\u001b[39;00m get_lib_fn(backend, fn)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\math\\single_dispatch.py:419\u001b[0m, in \u001b[0;36m_to_numpy_torch\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mis_conj\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m x\u001b[39m.\u001b[39mis_conj():\n\u001b[0;32m    417\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mresolve_conj()\n\u001b[1;32m--> 419\u001b[0m \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# The real iteration\n",
    "steps = 100\n",
    "n_iterations = 20\n",
    "learning_rate = 0.1\n",
    "# batch_size = 3\n",
    "\n",
    "generation_counter = 0\n",
    "\n",
    "model_name = f\"quGan-qu{n_wires}-quen{n_note_encoding}-step{steps}-iter{n_iterations}\"\n",
    "model_str = f\"{model_name}.pt\"\n",
    "\n",
    "if Path(model_str).is_file():\n",
    "    print(\"Loading model\")\n",
    "    generated_layer.load_state_dict(torch.load(model_str))\n",
    "    generated_layer.eval()\n",
    "else:\n",
    "    # print(\"Training model\")\n",
    "    for _ in range(steps):\n",
    "        discriminator_iteration(n_iterations, learning_rate)\n",
    "        # sync_weights(real_layer, generated_layer)\n",
    "        generator_iteration(n_iterations, learning_rate)\n",
    "    torch.save(generated_layer.state_dict(), model_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_notes(model, network_input, int_to_note, n_notes):\n",
    "    '''Generate notes from the neural network based on a sequence of notes'''\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    scale_factor = len(midi.int_to_note) / torch.max(midi.input_norms)\n",
    "    with torch.no_grad():\n",
    "        start = random.randint(0, len(network_input) - n_notes)\n",
    "\n",
    "        prediction_output = []\n",
    "\n",
    "\n",
    "        # generate n_notes\n",
    "\n",
    "        for i in range(start, start + n_notes):\n",
    "            input_ = network_input[i]\n",
    "            generated_note = model(shuffle_music(input_))\n",
    "            generated_note = (generated_note + 1) * midi.input_norms[i]\n",
    "            generated_note = int(generated_note)\n",
    "            counter = 1\n",
    "            while generated_note not in int_to_note:\n",
    "                generated_note *= counter/(counter+1)\n",
    "                generated_note = int(generated_note)\n",
    "                counter += 1\n",
    "\n",
    "            result = int_to_note[int(generated_note)]\n",
    "            prediction_output.append(result)\n",
    "\n",
    "        return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface='torch')\n",
    "def final_music_generator(inputs, note_weights):\n",
    "    music_generator_circuit(inputs, note_weights)\n",
    "    return measurement(n_note_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_only = qml.QNode(final_music_generator, dev, interface='torch')\n",
    "weight_gens = {\n",
    "    'note_weights':(n_gen_layers, n_note_encoding),\n",
    "}\n",
    "generator_only_layer = qml.qnn.TorchLayer(final_music_generator, weight_gens).to(running_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_final_weights(source_layer, target_layer):\n",
    "    '''Synchronize the weights of two layers'''\n",
    "    source_weights = source_layer.note_weights\n",
    "    target_weights = target_layer.note_weights\n",
    "    with torch.no_grad():\n",
    "        for source_weight, target_weight in zip(source_weights, target_weights):\n",
    "            target_weight.data = source_weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Notes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\math\\utils.py:61\u001b[0m, in \u001b[0;36mallclose\u001b[1;34m(a, b, rtol, atol, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# Some frameworks may provide their own allclose implementation.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# Try and use it if available.\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mallclose(a, b, rtol\u001b[39m=\u001b[39mrtol, atol\u001b[39m=\u001b[39matol, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m, \u001b[39mImportError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m     \u001b[39m# Otherwise, convert the input to NumPy arrays.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39m#    np.abs(a - b) <= atol + rtol * np.abs(b)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m#\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\autoray\\autoray.py:80\u001b[0m, in \u001b[0;36mdo\u001b[1;34m(fn, like, *args, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m backend \u001b[39m=\u001b[39m choose_backend(fn, \u001b[39m*\u001b[39margs, like\u001b[39m=\u001b[39mlike, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 80\u001b[0m \u001b[39mreturn\u001b[39;00m get_lib_fn(backend, fn)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: allclose(): argument 'other' (position 2) must be Tensor, not float",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sri Harsha\\IITB\\Autumn 2022\\Online\\Q-Magenta-Implementation\\QuGAN.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mGenerating Notes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sync_final_weights(generated_layer, generator_only_layer)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m notes \u001b[39m=\u001b[39m generate_notes(generator_only_layer, midi\u001b[39m.\u001b[39;49mnetwork_input, midi\u001b[39m.\u001b[39;49mint_to_note, n_notes\u001b[39m=\u001b[39;49mn_notes)\n",
      "\u001b[1;32mc:\\Sri Harsha\\IITB\\Autumn 2022\\Online\\Q-Magenta-Implementation\\QuGAN.ipynb Cell 19\u001b[0m in \u001b[0;36mgenerate_notes\u001b[1;34m(model, network_input, int_to_note, n_notes)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start, start \u001b[39m+\u001b[39m n_notes):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     input_ \u001b[39m=\u001b[39m network_input[i]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     generated_note \u001b[39m=\u001b[39m model(shuffle_music(input_))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     generated_note \u001b[39m=\u001b[39m (generated_note \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m midi\u001b[39m.\u001b[39minput_norms[i]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     generated_note \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(generated_note)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\qnn\\torch.py:313\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(reconstructor)\n\u001b[0;32m    312\u001b[0m \u001b[39m# If the input is 1-dimensional, calculate the forward pass as usual\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate_qnode(inputs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\qnn\\torch.py:328\u001b[0m, in \u001b[0;36mTorchLayer._evaluate_qnode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m\"\"\"Evaluates the QNode for a single input datapoint.\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \n\u001b[0;32m    318\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39m    tensor: output datapoint\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    324\u001b[0m kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    325\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_arg: x},\n\u001b[0;32m    326\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{arg: weight\u001b[39m.\u001b[39mto(x) \u001b[39mfor\u001b[39;00m arg, weight \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqnode_weights\u001b[39m.\u001b[39mitems()},\n\u001b[0;32m    327\u001b[0m }\n\u001b[1;32m--> 328\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqnode(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mtype(x\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\qnode.py:611\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    608\u001b[0m         set_shots(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_device, override_shots)(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_gradient_fn)()\n\u001b[0;32m    610\u001b[0m \u001b[39m# construct the tape\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(args, kwargs)\n\u001b[0;32m    613\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecute_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    614\u001b[0m using_custom_cache \u001b[39m=\u001b[39m (\n\u001b[0;32m    615\u001b[0m     \u001b[39mhasattr\u001b[39m(cache, \u001b[39m\"\u001b[39m\u001b[39m__getitem__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    616\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(cache, \u001b[39m\"\u001b[39m\u001b[39m__setitem__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    617\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(cache, \u001b[39m\"\u001b[39m\u001b[39m__delitem__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    618\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\qnode.py:526\u001b[0m, in \u001b[0;36mQNode.construct\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39mtape\u001b[39m.\u001b[39mQuantumTape()\n\u001b[0;32m    525\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtape:\n\u001b[1;32m--> 526\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qfunc_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    527\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39m_qfunc_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qfunc_output\n\u001b[0;32m    529\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtape\u001b[39m.\u001b[39mget_parameters(trainable_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Sri Harsha\\IITB\\Autumn 2022\\Online\\Q-Magenta-Implementation\\QuGAN.ipynb Cell 19\u001b[0m in \u001b[0;36mfinal_music_generator\u001b[1;34m(inputs, note_weights)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m@qml\u001b[39m\u001b[39m.\u001b[39mqnode(dev, interface\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinal_music_generator\u001b[39m(inputs, note_weights):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     music_generator_circuit(inputs, note_weights)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m measurement(n_note_encoding)\n",
      "\u001b[1;32mc:\\Sri Harsha\\IITB\\Autumn 2022\\Online\\Q-Magenta-Implementation\\QuGAN.ipynb Cell 19\u001b[0m in \u001b[0;36mmusic_generator_circuit\u001b[1;34m(inputs, note_weights)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmusic_generator_circuit\u001b[39m(inputs, note_weights):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     encode_music(inputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     music_generator(note_weights)\n",
      "\u001b[1;32mc:\\Sri Harsha\\IITB\\Autumn 2022\\Online\\Q-Magenta-Implementation\\QuGAN.ipynb Cell 19\u001b[0m in \u001b[0;36mencode_music\u001b[1;34m(notes)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_music\u001b[39m(notes):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     AmplitudeEmbedding(features\u001b[39m=\u001b[39;49mnotes, wires\u001b[39m=\u001b[39;49mencoding_range, normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\templates\\embeddings\\amplitude.py:129\u001b[0m, in \u001b[0;36mAmplitudeEmbedding.__init__\u001b[1;34m(self, features, wires, pad_with, normalize, do_queue, id)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_with \u001b[39m=\u001b[39m pad_with\n\u001b[0;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize \u001b[39m=\u001b[39m normalize\n\u001b[1;32m--> 129\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess(features, wires, pad_with, normalize)\n\u001b[0;32m    130\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(features, wires\u001b[39m=\u001b[39mwires, do_queue\u001b[39m=\u001b[39mdo_queue, \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39mid\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\templates\\embeddings\\amplitude.py:230\u001b[0m, in \u001b[0;36mAmplitudeEmbedding._preprocess\u001b[1;34m(features, wires, pad_with, normalize)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m normalize \u001b[39mor\u001b[39;00m pad_with:\n\u001b[0;32m    228\u001b[0m         feature_set \u001b[39m=\u001b[39m feature_set \u001b[39m/\u001b[39m qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39msqrt(norm)\n\u001b[1;32m--> 230\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m qml\u001b[39m.\u001b[39;49mmath\u001b[39m.\u001b[39;49mallclose(norm, \u001b[39m1.0\u001b[39;49m, atol\u001b[39m=\u001b[39;49mTOLERANCE):\n\u001b[0;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m normalize \u001b[39mor\u001b[39;00m pad_with:\n\u001b[0;32m    232\u001b[0m         feature_set \u001b[39m=\u001b[39m feature_set \u001b[39m/\u001b[39m qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39msqrt(norm)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\math\\utils.py:70\u001b[0m, in \u001b[0;36mallclose\u001b[1;34m(a, b, rtol, atol, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m     res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mallclose(a, b, rtol\u001b[39m=\u001b[39mrtol, atol\u001b[39m=\u001b[39matol, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m, \u001b[39mImportError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m     \u001b[39m# Otherwise, convert the input to NumPy arrays.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39m#    np.abs(a - b) <= atol + rtol * np.abs(b)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     t1 \u001b[39m=\u001b[39m ar\u001b[39m.\u001b[39;49mto_numpy(a)\n\u001b[0;32m     71\u001b[0m     t2 \u001b[39m=\u001b[39m ar\u001b[39m.\u001b[39mto_numpy(b)\n\u001b[0;32m     72\u001b[0m     res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mallclose(t1, t2, rtol\u001b[39m=\u001b[39mrtol, atol\u001b[39m=\u001b[39matol, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\autoray\\autoray.py:373\u001b[0m, in \u001b[0;36mto_numpy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_numpy\u001b[39m(x):\n\u001b[0;32m    372\u001b[0m     \u001b[39m\"\"\"Get a numpy version of array ``x``.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mreturn\u001b[39;00m do(\u001b[39m\"\u001b[39;49m\u001b[39mto_numpy\u001b[39;49m\u001b[39m\"\u001b[39;49m, x)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\autoray\\autoray.py:80\u001b[0m, in \u001b[0;36mdo\u001b[1;34m(fn, like, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m\"\"\"Do function named ``fn`` on ``(*args, **kwargs)``, peforming single\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mdispatch to retrieve ``fn`` based on whichever library defines the class of\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mthe ``args[0]``, or the ``like`` keyword argument if specified.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39m    <tf.Tensor: id=91, shape=(3, 3), dtype=float32>\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m backend \u001b[39m=\u001b[39m choose_backend(fn, \u001b[39m*\u001b[39margs, like\u001b[39m=\u001b[39mlike, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 80\u001b[0m \u001b[39mreturn\u001b[39;00m get_lib_fn(backend, fn)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pennylane\\math\\single_dispatch.py:419\u001b[0m, in \u001b[0;36m_to_numpy_torch\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mis_conj\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m x\u001b[39m.\u001b[39mis_conj():\n\u001b[0;32m    417\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mresolve_conj()\n\u001b[1;32m--> 419\u001b[0m \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "n_notes = 200\n",
    "generated_notes = []\n",
    "print('Generating Notes')\n",
    "sync_final_weights(generated_layer, generator_only_layer)\n",
    "notes = generate_notes(generator_only_layer, midi.network_input, midi.int_to_note, n_notes=n_notes)\n",
    "# notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving as MIDI file\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'notes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sri Harsha\\IITB\\Autumn 2022\\Online\\Q-Magenta-Implementation\\QuGAN.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m generation_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSaving as MIDI file\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sri%20Harsha/IITB/Autumn%202022/Online/Q-Magenta-Implementation/QuGAN.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m midi\u001b[39m.\u001b[39mcreate_midi_from_model(notes, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m_generated_\u001b[39m\u001b[39m{\u001b[39;00mgeneration_counter\u001b[39m}\u001b[39;00m\u001b[39m.mid\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'notes' is not defined"
     ]
    }
   ],
   "source": [
    "generation_counter += 1\n",
    "print('Saving as MIDI file')\n",
    "midi.create_midi_from_model(notes, f'{model_name}_generated_{generation_counter}.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
